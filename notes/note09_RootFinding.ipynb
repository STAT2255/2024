{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df6a38ca-b0b0-4406-a35e-a9036d2576f2",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "In statistics, we often have to deal with optimization problems where we need to find solution(s) that maximize or minimize a function. \n",
    "\n",
    "$x_{max} = \\underset{x}{\\mathrm{arg\\,max}}f(x) = \\{x|f(x) = \\underset{x'}{\\mathrm{max}}f(x')\\}\\quad \\text{or}\\quad x_{min} = \\underset{x}{\\mathrm{arg\\,min}}f(x) = \\{x|f(x) = \\underset{x'}{\\mathrm{min}}f(x')\\}$\n",
    "\n",
    "- The function we want to maximize (or minimize) sometimes are also called target or objective function.\n",
    "- The solution that maximize (or minimize) are called maximizer (or minimizer).\n",
    "- Without loss of generality, in this note, we will just focus on maximization problem.\n",
    "\n",
    "Typically, maximization of $f(x)$ is done by solving\n",
    "$$f'(x) = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c5ed03",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimator (MLE)\n",
    "\n",
    "### A simple example: Coin tosses\n",
    "\n",
    "-   Tossing a coin have binary outcomes: a Head (H) or a Tail (T).\n",
    "-   Let the probability of getting a Head be $p$ and the probability of\n",
    "getting a Tail be $1-p$.\n",
    "-   Let's toss the coin five times, and assume that we get the following sequence: H,T,T,H,H. \n",
    "-   How do we find the value of $p$?\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "The probability of seeing this result is\n",
    "$$L(p) = p(1-p)(1-p)pp = p^3(1-p)^2$$\n",
    "where 3 is the number of Heads and\n",
    "2 is the number of Tails.\n",
    "\n",
    "More generally, if we have a total of $N$ tosses, out of which $n$ are\n",
    "Heads, then the probability is written in a generic function form:\n",
    "$$L(p) = p^n(1-p)^{N-n}$$\n",
    "\n",
    "-   Here, $L(p)$ is the likelihood of observing the data.\n",
    "-   It is a function of the unknown parameter $p$.\n",
    "-   We want to use the maximizer of $L(p)$ to estimate $p$.\n",
    "-   The maximum likelihood estimator (MLE) is\n",
    "    $$\\hat{p}=\\arg\\max_{p}L(p).$$\n",
    "-   The estimation problem now becomes an optimization problem.\n",
    "\n",
    "### Log-likelihood\n",
    "\n",
    "The maximizer of $L(P)$ is the same as the maximizer of\n",
    "$$l(p) = \\log\\{L(p)\\}= n\\log p + (N-n)\\log(1-p).$$\n",
    "\n",
    "Thus $\\hat{p}$ is the solution to\n",
    "    $$l'(p)= \\frac{\\partial l}{\\partial p}=\\frac{n}{p}-\\frac{N-n}{1-p}=0,$$\n",
    "    which is $$\\hat{p}=\\frac{n}{N}=\\frac{3}{5}=0.6.$$\n",
    "\n",
    "### Logistic regression\n",
    "\n",
    "Assume we have some covariate $x$ that can be used to predict the outcome, say $y\\in\\{H, T\\}$, through modeling $p$ by\n",
    "$$\n",
    "p_{x_i}(\\theta)\n",
    "=\\Pr(H\\mid x_i; \\theta)\n",
    "=\\Pr(y_i=1\\mid x_i; \\theta)\n",
    "= \\frac{e^{x_i^{T}\\theta}}{1+e^{x_i^{T}\\theta}}.\n",
    "$$\n",
    "\n",
    "For a given data $(x_{i},y_{i}), i=1, ...N$, the log-likelihood for\n",
    "$\\theta$ is\n",
    "$$\n",
    "l(\\theta) = \\sum_{i=1}^{N}\\{y_{i}\\log p_{x_{i}} + (1-y_{i})\\log(1-p_{x_{i}})\\}\\\\\n",
    "     = \\sum_{i=1}^{N}\\{y_{i}x_{i}^{T}\\theta + \\log(1+e^{x_{i}^{T}\\theta})\\},\n",
    "$$\n",
    "\n",
    "with derivative\n",
    "\n",
    "$$\n",
    "l'(\\theta)\n",
    "= \\sum_{i=1}^{N}\\Big(y_{i} - \\frac{e^{x_i^{T}\\theta}}{1+e^{x_i^{T}\\theta}}\\Big)x_i.\n",
    "$$\n",
    "\n",
    "How should we find MLE $\\hat{\\theta}=\\arg\\max_{p}l(\\theta)$ or $l'(\\hat{\\theta})=0$?\n",
    "\n",
    "<!-- ## Example: Maximum Likelihood Estimator (mle) -->\n",
    "\n",
    "<!-- Suppose you have observed data points $y_1, y_2, \\dots, y_n$ that are independent and from the same distribution with probability density function $f(y; \\theta)$, where $\\theta$ is called parameter. -->\n",
    "\n",
    "<!-- For example, for Normal distribution $N(\\theta, 1)$, the density function is -->\n",
    "\n",
    "<!-- $$f(y_i;\\theta) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{(y_i-\\theta)^2}{2}}.$$ -->\n",
    "\n",
    "<!-- When we have observed data $y_1, y_2, \\dots, y_n$ from a distribution, the function now depends only on the unknow parameter $\\theta$.  This function can be thought as a function that tell us how likely we observe the data $y_1, y_2, \\dots, y_n$ given each parameter value $\\theta$. So, it is also called the likelihood function: -->\n",
    "\n",
    "<!-- $$L(\\theta) = \\prod_{i=1}^n f(y_i; \\theta).$$ -->\n",
    "\n",
    "<!-- So, to estimate the unknow parameter $\\theta$, it is intuitive to use the value of $\\theta$ that maximize the likelihood function. Maximizing $L(\\theta)$ is equivalent to maximizing $\\log L(\\theta)$, such that -->\n",
    "\n",
    "<!-- $$\\log L(\\theta) = l(\\theta) = \\log \\left(\\prod_{i=1}^n f(y_i; \\theta)\\right) = \\sum_{i=1}^n \\log f(y_i; \\theta)$$ -->\n",
    "\n",
    "<!-- To solve this problem, all we need to do is to solve the following problem -->\n",
    "\n",
    "<!-- $$l'(\\theta) = 0, \\quad \\text{where } l'(\\theta) = \\frac{d}{d\\theta}l(\\theta).$$ -->\n",
    "\n",
    "<!-- The solution (root) to the above equation, denoted as $\\hat \\theta$, is called the maximum likelihood estimator. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ff26d",
   "metadata": {},
   "source": [
    "# Root Finding Problem\n",
    "\n",
    "The root or zero of a function, $f(x)$, is an $x_0$ such that $ùëì\\,(ùë•_0)=0$\n",
    "\n",
    "For simple function, it is easy to find the solution. For example, with\n",
    "\n",
    "$$f(x) = 5x + 6,$$\n",
    "$x_0=-6/5$.\n",
    "\n",
    "For quadratic function, for example\n",
    "\n",
    "$$f(x) = 5x^2 + 8x + 1,$$\n",
    "\n",
    "we have the well-known result that states the roots are\n",
    "\n",
    "$$x_0=\\frac{-8 \\pm \\sqrt{64-4*5*1}}{2*5}.$$\n",
    "\n",
    "However, for other function such as $f(ùë•)= \\cos(x)‚àíx$, determining an analytic, or exact, solution for the roots of functions can be difficult. For these cases, it is useful to generate numerical approximations of the roots of f ‚Äì this is where root-finding algorithms come in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5cbc82",
   "metadata": {},
   "source": [
    "# General Structure\n",
    "\n",
    "1. start with an initial guess\n",
    "2. calculate the result of the guess\n",
    "3. update the guess based on the result and some further conditions\n",
    "4. repeat until you‚Äôre satisfied with the result (stopping rules)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d629922a",
   "metadata": {},
   "source": [
    "# Tolerance\n",
    "\n",
    "- **Error** is a deviation from the desired value to the computed or achieved value.\n",
    "- **Tolerance** is the level of error that is acceptable for an application.\n",
    "- We say that a computer program has **converged** to a solution when it has found a solution with an error smaller than the tolerance.\n",
    "\n",
    "For root finding problems, there are different choices for the measure of error, some examples are:\n",
    "\n",
    "1. Since we want $x^*$, such that $f\\,(x^*)$ is very close to 0. Therefore $|f\\,(x^*)|$ is a possible choice for the measure of error since the smaller it is, the closer it is to a root.\n",
    "\n",
    "$$\\text{Stopping rule: }\\text{stop if } |f\\,(x^*)|<\\epsilon, \\text{ where } \\epsilon \\text{ is pre-specified tolerance.}$$\n",
    "\n",
    "\n",
    "2. Also if we assume that $x^{(i)}$ is the $i$-th guess of an algorithm for finding a root, then $|x^{(i+1)}-x^{(i)}|$ is another possible choice for measuring error, since we expect the improvements between subsequent guesses to diminish as it approaches a solution\n",
    "\n",
    "$$\\text{Stopping rule: }\\text{stop if } |x^{(i+1)}-x^{(i)}|<\\epsilon, \\text{ where } \\epsilon \\text{ is pre-specified tolerance.}$$\n",
    "\n",
    "\n",
    "3. Using the relative improvement is another option:\n",
    "\n",
    "$$\\text{Stopping rule: }\\text{stop if } \\frac{|x^{(i+1)}-x^{(i)}|}{|x^{(i)}|}<\\epsilon, \\text{ where } \\epsilon \\text{ is pre-specified tolerance.}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505275b0",
   "metadata": {},
   "source": [
    "# Root Finding Algorithm\n",
    "\n",
    "Say we would like to maximize function \n",
    "\n",
    "$$f(x) = \\sin(x) - \\frac{x^2}{2}+6x.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10508a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 100 linearly spaced numbers\n",
    "x = np.linspace(0.5,10,100)\n",
    "\n",
    "# the function, which is y = log(x)/(1+x) here\n",
    "y = np.sin(x)-x**2/2 + 6*x\n",
    "\n",
    "# setting the axes\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['bottom'].set_position((\"data\", 3))\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "# plot the function\n",
    "plt.plot(x,y, 'r')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c946b1",
   "metadata": {},
   "source": [
    "Or finding the root of the derivative of $f(x)$,\n",
    "\n",
    "$$f'(x) = \\cos(x) - x + 6$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17501586",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 100 linearly spaced numbers\n",
    "x = np.linspace(0.7,10,100)\n",
    "\n",
    "# the function, which is y = log(x)/(1+x) here\n",
    "y = np.cos(x)-x+6\n",
    "\n",
    "# setting the axes\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.spines['left'].set_position((\"data\", 0.7))\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "# plot the function\n",
    "plt.plot(x,y, 'r')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963976b4",
   "metadata": {},
   "source": [
    "## Bisection Method\n",
    "\n",
    "> The **Intermediate Value Theorem** states that if $f$ is a continuous function whose domain contains the interval $[a, b]$ and N is a number between $f(a)$ and $f(b)$, then there exist $c\\in[a,b]$ such that $f(c)=N$.\n",
    "\n",
    "From this, we have the fact that \n",
    "> If a continuous function has values of opposite sign inside an interval, then it has a root in that interval (**Bolzano's Theorem**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab6b19c",
   "metadata": {},
   "source": [
    "You Bisection function takes args:\n",
    "- `f`,\n",
    "- `a`, \n",
    "- `b`, \n",
    "- `epsilon = 10**(-5)`, \n",
    "- `max_iter=100`\n",
    "\n",
    "initialize counter = 0\n",
    "\n",
    "**repeat** the following for max_iter iterations:\n",
    "\n",
    "   1. Calculate the mid-point between `a` and `b`, denote as `m`. \n",
    "   2. Plug `m` into the function `f` to see if `m` satisfy the tolerance:\n",
    "       - if yes, **return** `m` as the approximated root.\n",
    "   3. `if f(a)f(m) < 0:`\\\n",
    "           `b = m`\\\n",
    "       `else:`\\\n",
    "           `a = m`\n",
    "   4. update counter\n",
    "\n",
    "Here, we will use $|f\\,(m)|<\\epsilon$ as the stopping rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ec3bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisection1(f, a, b, eps = 10**(-5), max_iter = 100):\n",
    "    counter = 0\n",
    "    while counter < max_iter:\n",
    "        m = (a + b) / 2\n",
    "        fm = f(m)\n",
    "        if abs(fm) <= eps:\n",
    "            return m\n",
    "        if f(a) * fm < 0:\n",
    "            b = m\n",
    "        else:\n",
    "            a = m\n",
    "        counter += 1\n",
    "    print(\"Fail to converge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b80da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def y(x):\n",
    "    y = np.cos(x)-x+6\n",
    "    return y\n",
    "\n",
    "bisection1(y, 2, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0530d3e-796f-4d79-a127-b0ad6150f346",
   "metadata": {},
   "source": [
    "## Use `for` loop\n",
    "\n",
    "You Bisection function takes args: f, a, b, epsilon = 10**(-5), max_iter=100\n",
    "\n",
    "start with initial guess: m = (a+b)/2\n",
    "\n",
    "initialize counter = 0\n",
    "\n",
    "**repeat** the following until converge or reach max_iter iterations:\n",
    "\n",
    "  1. if f(a)f(m) < 0:\\\n",
    "         b = m\\\n",
    "     else:\\\n",
    "         a = m\n",
    "  2. Calculate the new mid-point between a and b, and update m. \n",
    "  3. Update counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd1d16de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisection2(f, a, b, eps = 10**(-5), max_iter = 100):\n",
    "    for i in range(max_iter):\n",
    "        m = (a + b) / 2\n",
    "        fm = f(m)\n",
    "        if abs(fm) <= eps:\n",
    "            return m\n",
    "        if f(a) * fm < 0:\n",
    "            b = m\n",
    "        else:\n",
    "            a = m\n",
    "    print(\"Fail to converge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f1b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bisection2(y, 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ce799-8640-4d10-ac38-6a507d6b4111",
   "metadata": {},
   "source": [
    "Use recursive function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca5cf611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisection3(f, a, b, eps= 10**(-5)):\n",
    "    m = (a+b)/2\n",
    "    counter = 0\n",
    "    if abs(f(m)) <= eps:\n",
    "        return m\n",
    "    if f(a)*f(m) < 0:\n",
    "        return (bisection3(f, a, m))\n",
    "    else:\n",
    "        return (bisection3(f, m, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f11e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bisection3(y, 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b4b05-f534-4337-845f-9f2ee1841fca",
   "metadata": {},
   "source": [
    "Test the above methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17f69e1-5ce7-4dd0-ab82-0fa4588759f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bisection1(y, 2, 10), bisection2(y, 2, 10), bisection3(y, 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda384f4-819a-47c6-933c-cfeee720462b",
   "metadata": {},
   "source": [
    "## Remark\n",
    " \n",
    "- You're likely to make mistakes in your first attempt at any algorithm.\n",
    "- To help you find and fix errors, add some diagnostic printing to your function that prints out a, b, and f(m) at each iteration.\n",
    "- Add an optional argument **diagnostics** to the function that turns the printing on and off. \n",
    "- Make diagnostics default to **False**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec4683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisection(f, a, b, eps = 10**(-5), max_iter = 100, diag = False):\n",
    "    counter = 0\n",
    "    while counter < max_iter:\n",
    "        m = (a + b) / 2\n",
    "        fm = f(m)\n",
    "        if diag:\n",
    "            print(m)\n",
    "        if abs(fm) <= eps:\n",
    "            return m\n",
    "        if f(a) * fm < 0:\n",
    "            b = m\n",
    "        else:\n",
    "            a = m\n",
    "        counter += 1\n",
    "    print(\"Fail to converge\")\n",
    "\n",
    "bisection(y, 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcecbed6",
   "metadata": {},
   "source": [
    "## Newton methods\n",
    "\n",
    "Let $f(x)$ be a continuous and differentiable function and $x_0$ be an root such that $f(x_0) = 0$. \n",
    "\n",
    "Starting with an initial guess, the Newton method is an iterative root finding approach which uses information about the current guess of the root $x^{(i)}$ and derivative of the function to produce subsequently better approximations of the root $x^{(i+1)}$.\n",
    "\n",
    "Given current guess $x^{(i)}$, using Taylor expansion to take the linear approximation of $f(x)$ around $x^{(i)}$, such that\n",
    "\n",
    "$$f(x) \\approx f(x^{(i)}) + f'(x^{(i)})(x-x^{(i)}),$$\n",
    "\n",
    "where $f'(x) = \\frac{d}{dx}f(x)$ is the derivative of $f$. Then, setting this approximation equal to 0, we can obtain the value of the next guess $x^{(t+1)}$, such that\n",
    "\n",
    "$$x^{(i+1)} = x^{(i)} - \\frac{f(x^{(i)})}{f'(x^{(i)})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51662860",
   "metadata": {},
   "source": [
    "You newton function takes args:\n",
    "- `f`,\n",
    "- `f_prime`,\n",
    "- `init`,\n",
    "- `epsilon = 10**(-5)`,\n",
    "- `max_iter=100`\n",
    "\n",
    "x_old = init\\\n",
    "**repeat** the following for max_iter iterations.\n",
    "   1. x_new = x_old - f(x_old)/f'(x_old)\n",
    "   2. Calculate error and compare with tolerance\n",
    "      - if converge, **return** x_new\n",
    "   3. x_old = x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f14dd247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(f, f_prime, init, epsilon=10**(-5), max_iter=100):\n",
    "    x_old = init\n",
    "    counter = 0\n",
    "    while counter < max_iter:\n",
    "        try:\n",
    "            x_new = x_old - f(x_old) / f_prime(x_old)\n",
    "        except ZeroDivisionError:\n",
    "            print(\"f_prime eveluate to 0 during iteration\")\n",
    "            return None\n",
    "        if (abs(x_new-x_old) < epsilon):\n",
    "            return x_new\n",
    "        x_old = x_new\n",
    "        counter += 1\n",
    "    print(\"Fail to get converged result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e9570-5fb4-4eab-93c1-f380d048835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yp(x):\n",
    "    yp = -np.sin(x) - 1\n",
    "    return yp\n",
    "\n",
    "newton(y, yp, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ca7b5",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "* Make sure to update x_old so the next iteration will use the most recent guess.\n",
    "* Here, we use $|x^{(i+1)}-x^{(i)}|<\\epsilon$ as the stopping rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbda681",
   "metadata": {},
   "source": [
    "## Fixed point method\n",
    "\n",
    "> A fixed point of a function is a point whose evaluation by that function equals to itself, i.e., $x = G(x)$.\n",
    "\n",
    "The natural way to hunt for the fixed point is to use the iterative method such that\n",
    "\n",
    "$$x^{(i+1)} = G(x^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636cbe11",
   "metadata": {},
   "source": [
    "## Root finding using fixed point iteration\n",
    "\n",
    "For solving f(x) = 0, we can simply let $G(x) = x + \\alpha f(x)$, where $\\alpha\\ne0$ is a constant.\n",
    "\n",
    "So, the next update is \n",
    "\n",
    "$$x^{(i+1)} = G(x^{(i)}) = x^{(i)} + \\alpha f(x^{(i)})$$\n",
    "\n",
    "According to the above, we can see Newton method is a special case of the fixed point method when $\\alpha = -1/f'(x^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2268af4",
   "metadata": {},
   "source": [
    "Your fix_point function takes args:\n",
    "- f\n",
    "- init\n",
    "- alpha\n",
    "- epsilon = 10**(-5)\n",
    "- max_iter=100\n",
    "\n",
    "x_old = init\\\n",
    "**repeat** the following for max_iter iterations.\n",
    "   1. x_new = x_old + alpha*f(x_old)\n",
    "   2. Calculate error and compare with tolerance\n",
    "      - if converge, **return** x_new\n",
    "   3. x_old = x_new\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat2255",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
